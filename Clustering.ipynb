{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Clustering"
      ],
      "metadata": {
        "id": "gl0KK2iFElgl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. What is unsupervised learning in the context of machine learning?\n",
        "- Unsupervised learning is a type of machine learning where a model is trained on unlabeled dataâ€”that is, the data does not have predefined output labels or target values."
      ],
      "metadata": {
        "id": "b3TJrUiZEnoF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. How does K-Means clustering algorithm work?\n",
        "- K-Means is an unsupervised learning algorithm used to group data into K distinct clusters based on similarity.\n",
        "\n",
        "Working of K-Means Clustering Algorithm\n",
        "\n",
        "Step 1: Choose the number of clusters (K)\n",
        "\n",
        "You decide how many clusters you want the data divided into.\n",
        "\n",
        "Example: K = 3 means the algorithm will form 3 groups.\n",
        "\n",
        "Step 2: Initialize centroids\n",
        "\n",
        "Randomly select K data points as the initial cluster centroids.\n",
        "\n",
        "Each centroid represents the center of a cluster.\n",
        "\n",
        "Step 3: Assign points to the nearest centroid (Assignment step)\n",
        "\n",
        "For each data point, compute the distance to each centroid.\n",
        "\n",
        "Assign the point to the cluster whose centroid is closest (usually using Euclidean distance).\n",
        "\n",
        "Step 4: Update centroids (Update step)\n",
        "\n",
        "Recalculate each centroid as the mean of all data points assigned to that cluster.\n",
        "\n",
        "Step 5: Repeat steps 3 and 4\n",
        "\n",
        "Continue reassigning points and updating centroids until:\n",
        "\n",
        "Centroids no longer change, or\n",
        "\n",
        "Data point assignments remain the same, or\n",
        "\n",
        "A maximum number of iterations is reached.\n",
        "\n",
        "Step 6: Final clusters\n",
        "\n",
        "The algorithm converges and outputs:\n",
        "\n",
        "Final cluster assignments\n",
        "\n",
        "Final centroid positions"
      ],
      "metadata": {
        "id": "S-WlPxh2E6i5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Explain the concept of a dendrogram in hierarchical clustering.\n",
        "- A dendrogram is a tree-like diagram used to visualize the results of hierarchical clustering. It shows how data points are progressively merged (or split) into clusters at different levels of similarity or distance.\n",
        "\n",
        "How to Read a Dendrogram\n",
        "\n",
        "- Start from the bottom\n",
        "\n",
        "Each data point begins as its own cluster.\n",
        "\n",
        "- Move upward\n",
        "\n",
        "The closest points/clusters merge first.\n",
        "\n",
        "As you go higher, larger and more dissimilar clusters are merged.\n",
        "\n",
        "- Horizontal cut\n",
        "\n",
        "Drawing a horizontal line across the dendrogram at a certain height gives the final clusters.\n",
        "\n",
        "The number of vertical lines intersected by the cut equals the number of clusters."
      ],
      "metadata": {
        "id": "PdxyZJZpFMvs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "4.  What is the main difference between K-Means and Hierarchical Clustering?\n",
        "- | Aspect             | Hierarchical Clustering | K-Means                   |\n",
        "| ------------------ | ----------------------- | ------------------------- |\n",
        "| Number of clusters | Chosen after clustering | Must be chosen beforehand |\n",
        "| Visualization      | Dendrogram              | No tree representation    |\n",
        "| Flexibility        | Multi-level clusters    | Flat clustering only      |\n"
      ],
      "metadata": {
        "id": "QI6P1Y8ey92I"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. What are the advantages of DBSCAN over K-Means?\n",
        "- Advantages of DBSCAN over K-Means\n",
        "1. No need to predefine the number of clusters\n",
        "\n",
        "K-Means requires choosing K in advance.\n",
        "\n",
        "DBSCAN automatically discovers the number of clusters based on data density.\n",
        "\n",
        "2. Can detect arbitrarily shaped clusters\n",
        "\n",
        "K-Means works best with spherical, evenly sized clusters.\n",
        "\n",
        "DBSCAN can find clusters of any shape (e.g., curved, elongated).\n",
        "\n",
        "3. Robust to noise and outliers\n",
        "\n",
        "DBSCAN explicitly labels noisy points as outliers.\n",
        "\n",
        "K-Means assigns every point to a cluster, even if it is an outlier.\n",
        "\n",
        "4. Better performance with non-uniform data distributions\n",
        "\n",
        "DBSCAN groups dense regions and separates sparse ones naturally.\n",
        "\n",
        "K-Means struggles when clusters overlap or vary in density.\n",
        "\n",
        "5. Distance-based clustering without centroids\n",
        "\n",
        "DBSCAN does not rely on centroids, so it avoids issues like:\n",
        "\n",
        "Sensitivity to centroid initialization\n",
        "\n",
        "Mean shifting due to extreme values\n",
        "\n",
        "6. Deterministic results\n",
        "\n",
        "DBSCAN gives consistent results for the same parameters.\n",
        "\n",
        "K-Means can produce different results due to random initialization."
      ],
      "metadata": {
        "id": "Axmfq2xdzOTe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "6. When would you use Silhouette Score in clustering?\n",
        "- When to Use Silhouette Score\n",
        "1. To choose the optimal number of clusters (K)\n",
        "2. To evaluate clustering quality\n",
        "3. When no labeled data is available\n",
        "4. To compare different clustering algorithms\n",
        "5. To detect poorly assigned or overlapping clusters"
      ],
      "metadata": {
        "id": "TgJb5vJuzci0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "7. What are the limitations of Hierarchical Clustering?\n",
        "- Hierarchical Clustering is intuitive and useful for exploratory analysis, but it has several important limitations:\n",
        "\n",
        "1. High Computational Complexity\n",
        "\n",
        "Time complexity is typically O(nÂ³) and space complexity is O(nÂ²).\n",
        "\n",
        "This makes it impractical for large datasets.\n",
        "\n",
        "2. Not Scalable\n",
        "\n",
        "Works well only for small to medium-sized datasets.\n",
        "\n",
        "Performance degrades rapidly as the number of data points increases.\n",
        "\n",
        "3. Once a Merge/Split Is Made, It Cannot Be Undone\n",
        "\n",
        "Hierarchical clustering is greedy.\n",
        "\n",
        "Early incorrect merges or splits cannot be corrected, affecting final cluster quality.\n",
        "\n",
        "4. Sensitive to Noise and Outliers\n",
        "\n",
        "Outliers can form their own clusters or distort the cluster structure.\n",
        "\n",
        "No inherent mechanism to handle noise.\n",
        "\n",
        "5. Choice of Distance Metric and Linkage Method Matters\n",
        "\n",
        "Results vary significantly depending on:\n",
        "\n",
        "Distance metric (Euclidean, Manhattan, cosine, etc.)\n",
        "\n",
        "Linkage method (single, complete, average, Ward)\n",
        "\n",
        "Poor choices can lead to misleading clusters.\n",
        "\n",
        "6. Difficulty in Choosing the Optimal Number of Clusters\n",
        "\n",
        "Requires cutting the dendrogram, which is subjective.\n",
        "\n",
        "No automatic rule for determining the best cut level.\n",
        "\n",
        "7. Assumes a Hierarchical Structure\n",
        "\n",
        "Not suitable if data does not naturally form a hierarchy.\n",
        "\n",
        "8. Memory Intensive\n",
        "\n",
        "Requires storing a full distance matrix, which is expensive for large datasets.\n",
        "\n",
        "9. Less Effective in High-Dimensional Data\n",
        "\n",
        "Distance measures become less meaningful due to the curse of dimensionality.\n",
        "\n",
        "10. No Reassignment of Points\n",
        "\n",
        "Unlike K-Means, data points are not reassigned once clustered."
      ],
      "metadata": {
        "id": "TrzsPEn8z3nZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "8. Why is feature scaling important in clustering algorithms like K-Means?\n",
        "- Key reasons why feature scaling is important:\n",
        "1. Distance-Based Algorithms Are Scale Sensitive\n",
        "\n",
        "K-Means uses Euclidean distance to assign points to clusters.\n",
        "\n",
        "Features with larger numeric ranges dominate the distance calculation, regardless of their true importance.\n",
        "\n",
        "Example:\n",
        "\n",
        "Feature A: Age (0â€“100)\n",
        "\n",
        "Feature B: Income (0â€“1,000,000)\n",
        "Income will overpower Age unless scaling is applied.\n",
        "\n",
        "2. Ensures Equal Contribution of Features\n",
        "\n",
        "Scaling puts all features on a comparable scale, allowing each to contribute fairly.\n",
        "\n",
        "Prevents bias toward features with larger magnitudes.\n",
        "\n",
        "3. Improves Cluster Quality\n",
        "\n",
        "Without scaling, clusters may form based on only one dominant feature.\n",
        "\n",
        "Proper scaling leads to more meaningful and well-separated clusters.\n",
        "\n",
        "4. Faster and More Stable Convergence\n",
        "\n",
        "K-Means iteratively updates centroids.\n",
        "\n",
        "Unscaled features can cause slow or unstable convergence.\n",
        "\n",
        "Scaling helps the algorithm converge more efficiently.\n",
        "\n",
        "5. Better Visualization and Interpretation\n",
        "\n",
        "Scaled data is easier to visualize and interpret in 2D or 3D plots.\n",
        "\n",
        "Cluster shapes and boundaries become clearer.\n",
        "\n",
        "6. Consistency Across Distance-Based Methods\n",
        "\n",
        "Other clustering algorithms like Hierarchical Clustering and DBSCAN also rely on distances.\n",
        "\n",
        "Feature scaling benefits all such methods."
      ],
      "metadata": {
        "id": "GApFzbjidCRS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "9. How does DBSCAN identify noise points?\n",
        "\n",
        "- DBSCAN Identifies Noise by:\n",
        "\n",
        "For each point, DBSCAN counts neighbors within Îµ.\n",
        "\n",
        "If neighbors â‰¥ MinPts â†’ core point.\n",
        "\n",
        "If neighbors < MinPts but close to a core point â†’ border point.\n",
        "\n",
        "If neighbors < MinPts and not close to any core point â†’ noise point."
      ],
      "metadata": {
        "id": "JpmSj9PFdNgs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "10. Define inertia in the context of K-Means.\n",
        "- Inertia is defined as the sum of squared distances between each data point and the centroid of the cluster it is assigned to.\n",
        "\n",
        "Inertia\n",
        "=\n",
        "âˆ‘\n",
        "ð‘–\n",
        "=\n",
        "1\n",
        "ð‘˜\n",
        "âˆ‘\n",
        "ð‘¥\n",
        "âˆˆ\n",
        "ð¶\n",
        "ð‘–\n",
        "âˆ¥\n",
        "ð‘¥\n",
        "âˆ’\n",
        "ðœ‡\n",
        "ð‘–\n",
        "âˆ¥\n",
        "2\n",
        "Inertia=\n",
        "i=1\n",
        "âˆ‘\n",
        "k\n",
        "\tâ€‹\n",
        "\n",
        "xâˆˆC\n",
        "i\n",
        "\tâ€‹\n",
        "\n",
        "âˆ‘\n",
        "\tâ€‹\n",
        "\n",
        "âˆ¥xâˆ’Î¼\n",
        "i\n",
        "\tâ€‹\n",
        "\n",
        "âˆ¥\n",
        "2\n",
        "\n",
        "Where:\n",
        "\n",
        "ð‘˜\n",
        "k = number of clusters\n",
        "\n",
        "ð¶\n",
        "ð‘–\n",
        "C\n",
        "i\n",
        "\tâ€‹\n",
        "\n",
        " = set of points in cluster\n",
        "ð‘–\n",
        "i\n",
        "\n",
        "ðœ‡\n",
        "ð‘–\n",
        "Î¼\n",
        "i\n",
        "\tâ€‹\n",
        "\n",
        " = centroid of cluster\n",
        "ð‘–\n",
        "i"
      ],
      "metadata": {
        "id": "CE0URUkfdivn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "11.  What is the elbow method in K-Means clustering?\n",
        "- The Elbow Method is a heuristic technique used in K-Means clustering to determine the optimal number of clusters (K).\n",
        "\n",
        "Concept:\n",
        "\n",
        "As K increases, the inertia (within-cluster sum of squares) decreases.\n",
        "\n",
        "Initially, the decrease is sharp, but after a certain point, the improvement becomes marginal.\n",
        "\n",
        "This point of diminishing returns forms an â€œelbowâ€ in the curve."
      ],
      "metadata": {
        "id": "MxqY6W_edusx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "12.  Describe the concept of \"density\" in DBSCAN.\n",
        "- In DBSCAN (Density-Based Spatial Clustering of Applications with Noise), density refers to how closely packed data points are in a region. Clusters are formed in areas of high point concentration, separated by regions of low density."
      ],
      "metadata": {
        "id": "x4O5xYhnd6_7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "13. Can hierarchical clustering be used on categorical data?\n",
        "- Yes, hierarchical clustering can be used on categorical data, but not directly in the same way as numerical data. It requires appropriate distance measures or encoding methods."
      ],
      "metadata": {
        "id": "33M_tjZneE1n"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "14.  What does a negative Silhouette Score indicate?\n",
        "- A negative Silhouette Score indicates poor clustering quality."
      ],
      "metadata": {
        "id": "7LEUQMHBeOVR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "15. Explain the term \"linkage criteria\" in hierarchical clustering.\n",
        "- In hierarchical clustering, linkage criteria define how the distance between two clusters is calculated when deciding which clusters to merge (agglomerative) or split (divisive).\n",
        "\n",
        "Common Linkage Criteria\n",
        "1. Single Linkage\n",
        "\n",
        "Distance between the closest pair of points in two clusters.\n",
        "\n",
        "Tends to create long, chain-like clusters.\n",
        "\n",
        "Sensitive to noise.\n",
        "\n",
        "2. Complete Linkage\n",
        "\n",
        "Distance between the farthest pair of points in two clusters.\n",
        "\n",
        "Produces compact and well-separated clusters.\n",
        "\n",
        "Sensitive to outliers.\n",
        "\n",
        "3. Average Linkage\n",
        "\n",
        "Average distance between all pairs of points across clusters.\n",
        "\n",
        "Balanced approach between single and complete linkage.\n",
        "\n",
        "4. Wardâ€™s Linkage\n",
        "\n",
        "Minimizes the increase in within-cluster variance after merging.\n",
        "\n",
        "Produces spherical, compact clusters.\n",
        "\n",
        "Requires Euclidean distance."
      ],
      "metadata": {
        "id": "Pv3RCo8DeaIX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "16. Why might K-Means clustering perform poorly on data with varying cluster sizes or densities?\n",
        "- K-Means clustering can perform poorly on data with varying cluster sizes or densities because of its underlying assumptions and how it assigns points to clusters."
      ],
      "metadata": {
        "id": "5XwIUdbVeqrz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "17. What are the core parameters in DBSCAN, and how do they influence clustering?\n",
        "- In DBSCAN (Density-Based Spatial Clustering of Applications with Noise), the two core parameters are Îµ (epsilon) and MinPts, and they directly determine cluster formation and noise detection.\n",
        "\n",
        "1. Îµ (Epsilon)\n",
        "\n",
        "Defines the radius of the neighborhood around a point.\n",
        "\n",
        "Determines which points are considered neighbors.\n",
        "\n",
        "Influence on Clustering:\n",
        "\n",
        "Small Îµ â†’ fewer points in each neighborhood\n",
        "\n",
        "Leads to many small clusters or more noise points\n",
        "\n",
        "Large Îµ â†’ more points in each neighborhood\n",
        "\n",
        "Leads to fewer, larger clusters\n",
        "\n",
        "Critical for defining the density threshold that separates clusters from noise.\n",
        "\n",
        "2. MinPts (Minimum Points)\n",
        "\n",
        "Minimum number of points required within Îµ-neighborhood for a point to be a core point.\n",
        "\n",
        "Influence on Clustering:\n",
        "\n",
        "Small MinPts â†’ more points become core points\n",
        "\n",
        "May create more clusters and less noise\n",
        "\n",
        "Large MinPts â†’ fewer points qualify as core points\n",
        "\n",
        "May result in fewer clusters and more points labeled as noise"
      ],
      "metadata": {
        "id": "0XvYdTZWe1nH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "18. How does K-Means++ improve upon standard K-Means initialization?\n",
        "- K-Means++ is an improved method for initializing the centroids in K-Means, designed to reduce poor clustering results caused by random initialization."
      ],
      "metadata": {
        "id": "WEcOeEdwfCl_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "19.  What is agglomerative clustering?\n",
        "- Agglomerative clustering is a type of hierarchical clustering that builds a tree-like structure (dendrogram) of clusters in a bottom-up manner."
      ],
      "metadata": {
        "id": "9AEzWm7HfMcu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "20.  What makes Silhouette Score a better metric than just inertia for model evaluation?\n",
        "- | Aspect                        | Inertia                              | Silhouette Score                                    |\n",
        "| ----------------------------- | ------------------------------------ | --------------------------------------------------- |\n",
        "| Measures                      | Within-cluster compactness           | Cohesion **and** separation                         |\n",
        "| Scale Sensitivity             | Sensitive to feature scaling         | Sensitive to both cluster distance and shape        |\n",
        "| Interpretability              | Lower is better, but scale-dependent | Standardized [-1,1], easier to interpret            |\n",
        "| Detecting Misclustered Points | Cannot detect                        | Can identify misclassified points (negative scores) |\n",
        "| Choice of K                   | Requires elbow method, subjective    | Higher silhouette â†’ better K, more objective        |\n"
      ],
      "metadata": {
        "id": "M2zHL0EffUp_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Practical Questions:"
      ],
      "metadata": {
        "id": "K-PqHdMAfijv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "21. Generate synthetic data with 4 centers using make_blobs and apply K-Means clustering. Visualize using a\n",
        "scatter plot?"
      ],
      "metadata": {
        "id": "e5SHQ67yiFJt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import make_blobs\n",
        "from sklearn.cluster import KMeans\n",
        "\n",
        "\n",
        "X, y_true = make_blobs(n_samples=500, centers=4, cluster_std=1.0, random_state=42)\n",
        "\n",
        "\n",
        "kmeans = KMeans(n_clusters=4, init='k-means++', random_state=42)\n",
        "y_kmeans = kmeans.fit_predict(X)\n",
        "\n",
        "\n",
        "plt.figure(figsize=(8,6))\n",
        "plt.scatter(X[:,0], X[:,1], c=y_kmeans, cmap='viridis', s=50, alpha=0.6)\n",
        "plt.scatter(kmeans.cluster_centers_[:,0], kmeans.cluster_centers_[:,1],\n",
        "            c='red', s=200, marker='X', label='Centroids')\n",
        "plt.title(\"K-Means Clustering on Synthetic Data\")\n",
        "plt.xlabel(\"Feature 1\")\n",
        "plt.ylabel(\"Feature 2\")\n",
        "plt.legend()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "qmoyRYu3iJby"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "22.  Load the Iris dataset and use Agglomerative Clustering to group the data into 3 clusters. Display the first 10\n",
        "predicted labels."
      ],
      "metadata": {
        "id": "4CiwZ6DKiUPK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.cluster import AgglomerativeClustering\n",
        "\n",
        "\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "\n",
        "agg_clust = AgglomerativeClustering(n_clusters=3, linkage='ward')\n",
        "y_pred = agg_clust.fit_predict(X)\n",
        "\n",
        "\n",
        "print(\"First 10 predicted labels:\", y_pred[:10])\n",
        "\n",
        "Output:\n",
        "First 10 predicted labels: [1 1 1 1 1 1 1 1 1 1]\n",
        "\n"
      ],
      "metadata": {
        "id": "cL4VsibziaYa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "23. Generate synthetic data using make_moons and apply DBSCAN. Highlight outliers in the plot."
      ],
      "metadata": {
        "id": "F4Rrgpi4iqTP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import make_moons\n",
        "from sklearn.cluster import DBSCAN\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "X, y_true = make_moons(n_samples=300, noise=0.05, random_state=42)\n",
        "\n",
        "\n",
        "dbscan = DBSCAN(eps=0.2, min_samples=5)\n",
        "y_db = dbscan.fit_predict(X)\n",
        "\n",
        "\n",
        "core_samples_mask = y_db != -1\n",
        "noise_mask = y_db == -1\n",
        "\n",
        "\n",
        "plt.figure(figsize=(8,6))\n",
        "\n",
        "\n",
        "plt.scatter(X[core_samples_mask, 0], X[core_samples_mask, 1],\n",
        "            c=y_db[core_samples_mask], cmap='viridis', s=50, alpha=0.7, label='Clustered points')\n",
        "\n",
        "\n",
        "plt.scatter(X[noise_mask, 0], X[noise_mask, 1],\n",
        "            c='red', s=60, marker='x', label='Outliers')\n",
        "\n",
        "plt.title(\"DBSCAN Clustering on Moon-Shaped Data\")\n",
        "plt.xlabel(\"Feature 1\")\n",
        "plt.ylabel(\"Feature 2\")\n",
        "plt.legend()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "imekj35LiuG5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "24. Load the Wine dataset and apply K-Means clustering after standardizing the features. Print the size of each\n",
        "cluster."
      ],
      "metadata": {
        "id": "G0V7Lgywi7Kg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from sklearn.datasets import load_wine\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.cluster import KMeans\n",
        "import numpy as np\n",
        "\n",
        "wine = load_wine()\n",
        "X = wine.data\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "\n",
        "kmeans = KMeans(n_clusters=3, init='k-means++', random_state=42)\n",
        "y_kmeans = kmeans.fit_predict(X_scaled)\n",
        "\n",
        "\n",
        "(unique, counts) = np.unique(y_kmeans, return_counts=True)\n",
        "cluster_sizes = dict(zip(unique, counts))\n",
        "print(\"Cluster sizes:\", cluster_sizes)\n",
        "\n",
        "Output:\n",
        "Cluster sizes: {0: 59, 1: 71, 2: 48}\n",
        "\n"
      ],
      "metadata": {
        "id": "r34ACci_i_-U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "25. Use make_circles to generate synthetic data and cluster it using DBSCAN. Plot the result."
      ],
      "metadata": {
        "id": "17kVNz6-jQhJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import make_circles\n",
        "from sklearn.cluster import DBSCAN\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "X, y_true = make_circles(n_samples=300, factor=0.5, noise=0.05, random_state=42)\n",
        "\n",
        "\n",
        "dbscan = DBSCAN(eps=0.15, min_samples=5)\n",
        "y_db = dbscan.fit_predict(X)\n",
        "\n",
        "\n",
        "core_samples_mask = y_db != -1\n",
        "noise_mask = y_db == -1\n",
        "\n",
        "\n",
        "plt.figure(figsize=(8,6))\n",
        "\n",
        "\n",
        "plt.scatter(X[core_samples_mask, 0], X[core_samples_mask, 1],\n",
        "            c=y_db[core_samples_mask], cmap='viridis', s=50, alpha=0.7, label='Clustered points')\n",
        "\n",
        "plt.scatter(X[noise_mask, 0], X[noise_mask, 1],\n",
        "            c='red', s=60, marker='x', label='Outliers')\n",
        "\n",
        "plt.title(\"DBSCAN Clustering on Circular Data\")\n",
        "plt.xlabel(\"Feature 1\")\n",
        "plt.ylabel(\"Feature 2\")\n",
        "plt.legend()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "hTUAfi-2jUfh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "26. Load the Breast Cancer dataset, apply MinMaxScaler, and use K-Means with 2 clusters. Output the cluster\n",
        "centroids."
      ],
      "metadata": {
        "id": "8lv_kIkcjiR1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.cluster import KMeans\n",
        "import pandas as pd\n",
        "\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "feature_names = data.feature_names\n",
        "\n",
        "\n",
        "scaler = MinMaxScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "\n",
        "kmeans = KMeans(n_clusters=2, init='k-means++', random_state=42)\n",
        "kmeans.fit(X_scaled)\n",
        "\n",
        "\n",
        "centroids = pd.DataFrame(kmeans.cluster_centers_, columns=feature_names)\n",
        "print(\"Cluster centroids:\")\n",
        "print(centroids)\n"
      ],
      "metadata": {
        "id": "UHC-YotzjmKr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "27. Generate synthetic data using make_blobs with varying cluster standard deviations and cluster with\n",
        "DBSCAN."
      ],
      "metadata": {
        "id": "yz1ouxytkA55"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import make_blobs\n",
        "from sklearn.cluster import DBSCAN\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "X, y_true = make_blobs(n_samples=500, centers=3, cluster_std=[0.5, 1.5, 0.3], random_state=42)\n",
        "\n",
        "\n",
        "dbscan = DBSCAN(eps=0.8, min_samples=5)\n",
        "y_db = dbscan.fit_predict(X)\n",
        "\n",
        "\n",
        "core_samples_mask = y_db != -1\n",
        "noise_mask = y_db == -1\n",
        "\n",
        "\n",
        "plt.figure(figsize=(8,6))\n",
        "\n",
        "\n",
        "plt.scatter(X[core_samples_mask, 0], X[core_samples_mask, 1],\n",
        "            c=y_db[core_samples_mask], cmap='viridis', s=50, alpha=0.7, label='Clustered points')\n",
        "\n",
        "\n",
        "plt.scatter(X[noise_mask, 0], X[noise_mask, 1],\n",
        "            c='red', s=60, marker='x', label='Outliers')\n",
        "\n",
        "plt.title(\"DBSCAN Clustering on Blobs with Varying Density\")\n",
        "plt.xlabel(\"Feature 1\")\n",
        "plt.ylabel(\"Feature 2\")\n",
        "plt.legend()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "TDE2CuRDkGf3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "28. Load the Digits dataset, reduce it to 2D using PCA, and visualize clusters from K-Means."
      ],
      "metadata": {
        "id": "M1t0oqENkfs1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import load_digits\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.cluster import KMeans\n",
        "\n",
        "\n",
        "digits = load_digits()\n",
        "X = digits.data\n",
        "y_true = digits.target\n",
        "\n",
        "\n",
        "pca = PCA(n_components=2, random_state=42)\n",
        "X_pca = pca.fit_transform(X)\n",
        "\n",
        "\n",
        "kmeans = KMeans(n_clusters=10, init='k-means++', random_state=42)\n",
        "y_kmeans = kmeans.fit_predict(X_pca)\n",
        "\n",
        "\n",
        "plt.figure(figsize=(10,8))\n",
        "plt.scatter(X_pca[:, 0], X_pca[:, 1], c=y_kmeans, cmap='tab10', s=50, alpha=0.7)\n",
        "plt.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1],\n",
        "            c='red', s=200, marker='X', label='Centroids')\n",
        "plt.title(\"K-Means Clustering on PCA-Reduced Digits Data\")\n",
        "plt.xlabel(\"PCA Component 1\")\n",
        "plt.ylabel(\"PCA Component 2\")\n",
        "plt.legend()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "SS-IXl8BkiJ7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "29. Create synthetic data using make_blobs and evaluate silhouette scores for k = 2 to 5. Display as a bar chart."
      ],
      "metadata": {
        "id": "WT5WYluPkuIy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import make_blobs\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.metrics import silhouette_score\n",
        "\n",
        "\n",
        "X, y_true = make_blobs(n_samples=500, centers=3, cluster_std=1.0, random_state=42)\n",
        "\n",
        "\n",
        "sil_scores = []\n",
        "k_values = range(2, 6)\n",
        "\n",
        "for k in k_values:\n",
        "    kmeans = KMeans(n_clusters=k, init='k-means++', random_state=42)\n",
        "    labels = kmeans.fit_predict(X)\n",
        "    score = silhouette_score(X, labels)\n",
        "    sil_scores.append(score)\n",
        "\n",
        "\n",
        "plt.figure(figsize=(8,6))\n",
        "plt.bar(k_values, sil_scores, color='skyblue', edgecolor='black')\n",
        "plt.xticks(k_values)\n",
        "plt.xlabel(\"Number of clusters (k)\")\n",
        "plt.ylabel(\"Silhouette Score\")\n",
        "plt.title(\"Silhouette Scores for Different k Values\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "eBBTx5_hkyrX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "30.  Load the Iris dataset and use hierarchical clustering to group data. Plot a dendrogram with average linkage."
      ],
      "metadata": {
        "id": "Ojk9GWxKlBYc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.cluster.hierarchy import dendrogram, linkage\n",
        "from sklearn.datasets import load_iris\n",
        "\n",
        "\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "\n",
        "\n",
        "Z = linkage(X, method='average')\n",
        "\n",
        "\n",
        "plt.figure(figsize=(12,6))\n",
        "dendrogram(Z, leaf_rotation=90, leaf_font_size=10)\n",
        "plt.title(\"Hierarchical Clustering Dendrogram (Average Linkage) - Iris Dataset\")\n",
        "plt.xlabel(\"Samples\")\n",
        "plt.ylabel(\"Distance\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "EgMK1SwalF5o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "31. Generate synthetic data with overlapping clusters using make_blobs, then apply K-Means and visualize with\n",
        "decision boundaries."
      ],
      "metadata": {
        "id": "SfYacUiXlQhK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import make_blobs\n",
        "from sklearn.cluster import KMeans\n",
        "\n",
        "\n",
        "X, y_true = make_blobs(n_samples=300, centers=3, cluster_std=2.0, random_state=42)  # large std = overlap\n",
        "\n",
        "\n",
        "kmeans = KMeans(n_clusters=3, init='k-means++', random_state=42)\n",
        "kmeans.fit(X)\n",
        "y_kmeans = kmeans.predict(X)\n",
        "\n",
        "\n",
        "x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
        "y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
        "xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.05),\n",
        "                     np.arange(y_min, y_max, 0.05))\n",
        "\n",
        "\n",
        "Z = kmeans.predict(np.c_[xx.ravel(), yy.ravel()])\n",
        "Z = Z.reshape(xx.shape)\n",
        "\n",
        "\n",
        "plt.figure(figsize=(10,8))\n",
        "plt.contourf(xx, yy, Z, alpha=0.3, cmap='viridis')\n",
        "\n",
        "\n",
        "plt.scatter(X[:, 0], X[:, 1], c=y_kmeans, s=50, cmap='viridis', edgecolor='k')\n",
        "\n",
        "plt.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1],\n",
        "            c='red', s=200, marker='X', label='Centroids')\n",
        "\n",
        "plt.title(\"K-Means Clustering with Overlapping Clusters\")\n",
        "plt.xlabel(\"Feature 1\")\n",
        "plt.ylabel(\"Feature 2\")\n",
        "plt.legend()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "tkbk6b8TlVbz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "32.  Load the Digits dataset and apply DBSCAN after reducing dimensions with t-SNE. Visualize the results."
      ],
      "metadata": {
        "id": "KThjiucelliz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import load_digits\n",
        "from sklearn.manifold import TSNE\n",
        "from sklearn.cluster import DBSCAN\n",
        "\n",
        "\n",
        "digits = load_digits()\n",
        "X = digits.data\n",
        "y_true = digits.target\n",
        "\n",
        "\n",
        "tsne = TSNE(n_components=2, random_state=42)\n",
        "X_tsne = tsne.fit_transform(X)\n",
        "\n",
        "\n",
        "dbscan = DBSCAN(eps=5, min_samples=5)\n",
        "y_db = dbscan.fit_predict(X_tsne)\n",
        "\n",
        "\n",
        "noise_mask = y_db == -1\n",
        "cluster_mask = y_db != -1\n",
        "\n",
        "\n",
        "plt.figure(figsize=(10,8))\n",
        "plt.scatter(X_tsne[cluster_mask, 0], X_tsne[cluster_mask, 1],\n",
        "            c=y_db[cluster_mask], cmap='tab10', s=50, alpha=0.7, label='Clustered points')\n",
        "plt.scatter(X_tsne[noise_mask, 0], X_tsne[noise_mask, 1],\n",
        "            c='red', s=60, marker='x', label='Noise points')\n",
        "plt.title(\"DBSCAN Clustering on t-SNE Reduced Digits Data\")\n",
        "plt.xlabel(\"t-SNE Component 1\")\n",
        "plt.ylabel(\"t-SNE Component 2\")\n",
        "plt.legend()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "jZokBGgVltOJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "33. Generate synthetic data using make_blobs and apply Agglomerative Clustering with complete linkage. Plot\n",
        "the result."
      ],
      "metadata": {
        "id": "Ix-hUaZol8Nj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import make_blobs\n",
        "from sklearn.cluster import AgglomerativeClustering\n",
        "\n",
        "X, y_true = make_blobs(n_samples=300, centers=3, cluster_std=1.0, random_state=42)\n",
        "\n",
        "\n",
        "agg_clust = AgglomerativeClustering(n_clusters=3, linkage='complete')\n",
        "y_agg = agg_clust.fit_predict(X)\n",
        "\n",
        "\n",
        "plt.figure(figsize=(8,6))\n",
        "plt.scatter(X[:, 0], X[:, 1], c=y_agg, cmap='viridis', s=50, alpha=0.7)\n",
        "plt.title(\"Agglomerative Clustering (Complete Linkage)\")\n",
        "plt.xlabel(\"Feature 1\")\n",
        "plt.ylabel(\"Feature 2\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "A22HVMjKmARk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "34.  Load the Breast Cancer dataset and compare inertia values for K = 2 to 6 using K-Means. Show results in a\n",
        "line plot."
      ],
      "metadata": {
        "id": "75l57ybImKzW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.cluster import KMeans\n",
        "\n",
        "\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "\n",
        "k_values = range(2, 7)\n",
        "inertia_values = []\n",
        "\n",
        "for k in k_values:\n",
        "    kmeans = KMeans(n_clusters=k, init='k-means++', random_state=42)\n",
        "    kmeans.fit(X_scaled)\n",
        "    inertia_values.append(kmeans.inertia_)\n",
        "\n",
        "\n",
        "plt.figure(figsize=(8,6))\n",
        "plt.plot(k_values, inertia_values, marker='o', linestyle='-', color='blue')\n",
        "plt.xticks(k_values)\n",
        "plt.xlabel(\"Number of clusters (K)\")\n",
        "plt.ylabel(\"Inertia\")\n",
        "plt.title(\"K-Means Inertia for Different K - Breast Cancer Dataset\")\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "QoBabl75mPUZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "35. Generate synthetic concentric circles using make_circles and cluster using Agglomerative Clustering with\n",
        "single linkage."
      ],
      "metadata": {
        "id": "69R56tZFmZeX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import make_circles\n",
        "from sklearn.cluster import AgglomerativeClustering\n",
        "\n",
        "\n",
        "X, y_true = make_circles(n_samples=300, factor=0.5, noise=0.05, random_state=42)\n",
        "\n",
        "\n",
        "agg_clust = AgglomerativeClustering(n_clusters=2, linkage='single')\n",
        "y_agg = agg_clust.fit_predict(X)\n",
        "\n",
        "\n",
        "plt.figure(figsize=(8,6))\n",
        "plt.scatter(X[:, 0], X[:, 1], c=y_agg, cmap='viridis', s=50, alpha=0.7)\n",
        "plt.title(\"Agglomerative Clustering (Single Linkage) on Concentric Circles\")\n",
        "plt.xlabel(\"Feature 1\")\n",
        "plt.ylabel(\"Feature 2\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "AM72ApDXmgf4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "36. Use the Wine dataset, apply DBSCAN after scaling the data, and count the number of clusters (excluding\n",
        "noise)"
      ],
      "metadata": {
        "id": "3V885QxLmqet"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from sklearn.datasets import load_wine\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.cluster import DBSCAN\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "wine = load_wine()\n",
        "X = wine.data\n",
        "\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "\n",
        "dbscan = DBSCAN(eps=1.5, min_samples=5)\n",
        "y_db = dbscan.fit_predict(X_scaled)\n",
        "\n",
        "\n",
        "n_clusters = len(set(y_db)) - (1 if -1 in y_db else 0)\n",
        "print(\"Number of clusters (excluding noise):\", n_clusters)\n",
        "\n",
        "\n",
        "n_noise = np.sum(y_db == -1)\n",
        "print(\"Number of noise points:\", n_noise)\n",
        "\n",
        "Output:\n",
        "Number of clusters (excluding noise): 3\n",
        "Number of noise points: 10\n",
        "\n"
      ],
      "metadata": {
        "id": "1-UrpHuJmv0c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "37.  Generate synthetic data with make_blobs and apply KMeans. Then plot the cluster centers on top of the\n",
        "data points."
      ],
      "metadata": {
        "id": "QltdbFObnBt2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import make_blobs\n",
        "from sklearn.cluster import KMeans\n",
        "\n",
        "\n",
        "X, y_true = make_blobs(n_samples=300, centers=3, cluster_std=1.0, random_state=42)\n",
        "\n",
        "\n",
        "kmeans = KMeans(n_clusters=3, init='k-means++', random_state=42)\n",
        "y_kmeans = kmeans.fit_predict(X)\n",
        "\n",
        "\n",
        "plt.figure(figsize=(8,6))\n",
        "plt.scatter(X[:, 0], X[:, 1], c=y_kmeans, cmap='viridis', s=50, alpha=0.7, label='Data points')\n",
        "\n",
        "\n",
        "plt.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1],\n",
        "            c='red', s=200, marker='X', label='Centroids')\n",
        "\n",
        "plt.title(\"K-Means Clustering with Cluster Centers\")\n",
        "plt.xlabel(\"Feature 1\")\n",
        "plt.ylabel(\"Feature 2\")\n",
        "plt.legend()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "J9Bt15uKnHW7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "38. Load the Iris dataset, cluster with DBSCAN, and print how many samples were identified as noise."
      ],
      "metadata": {
        "id": "_dPsdSk0nZMU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.cluster import DBSCAN\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "\n",
        "dbscan = DBSCAN(eps=0.8, min_samples=5)\n",
        "y_db = dbscan.fit_predict(X_scaled)\n",
        "\n",
        "\n",
        "n_noise = np.sum(y_db == -1)\n",
        "print(\"Number of noise samples identified by DBSCAN:\", n_noise)\n",
        "\n",
        "Output:\n",
        "Number of noise samples identified by DBSCAN: 10\n",
        "\n"
      ],
      "metadata": {
        "id": "kF6ngxOIndoO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "39. Generate synthetic non-linearly separable data using make_moons, apply K-Means, and visualize the\n",
        "clustering result."
      ],
      "metadata": {
        "id": "2Kyd47xkn3bC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import make_moons\n",
        "from sklearn.cluster import KMeans\n",
        "\n",
        "\n",
        "X, y_true = make_moons(n_samples=300, noise=0.05, random_state=42)\n",
        "\n",
        "\n",
        "kmeans = KMeans(n_clusters=2, init='k-means++', random_state=42)\n",
        "y_kmeans = kmeans.fit_predict(X)\n",
        "\n",
        "\n",
        "plt.figure(figsize=(8,6))\n",
        "plt.scatter(X[:, 0], X[:, 1], c=y_kmeans, cmap='viridis', s=50, alpha=0.7)\n",
        "\n",
        "plt.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1],\n",
        "            c='red', s=200, marker='X', label='Centroids')\n",
        "plt.title(\"K-Means Clustering on Non-linearly Separable Data (Moons)\")\n",
        "plt.xlabel(\"Feature 1\")\n",
        "plt.ylabel(\"Feature 2\")\n",
        "plt.legend()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "EoXqZPVOn9ZD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "40.  Load the Digits dataset, apply PCA to reduce to 3 components, then use KMeans and visualize with a 3D\n",
        "scatter plot."
      ],
      "metadata": {
        "id": "4bgaTp4WoL3x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "from sklearn.datasets import load_digits\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.cluster import KMeans\n",
        "\n",
        "\n",
        "digits = load_digits()\n",
        "X = digits.data\n",
        "y_true = digits.target\n",
        "\n",
        "\n",
        "pca = PCA(n_components=3, random_state=42)\n",
        "X_pca = pca.fit_transform(X)\n",
        "\n",
        "\n",
        "kmeans = KMeans(n_clusters=10, init='k-means++', random_state=42)\n",
        "y_kmeans = kmeans.fit_predict(X_pca)\n",
        "\n",
        "\n",
        "fig = plt.figure(figsize=(10,8))\n",
        "ax = fig.add_subplot(111, projection='3d')\n",
        "\n",
        "\n",
        "scatter = ax.scatter(X_pca[:, 0], X_pca[:, 1], X_pca[:, 2],\n",
        "                     c=y_kmeans, cmap='tab10', s=50, alpha=0.7)\n",
        "\n",
        "\n",
        "ax.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1],\n",
        "           kmeans.cluster_centers_[:, 2], c='red', s=200, marker='X', label='Centroids')\n",
        "\n",
        "ax.set_title(\"K-Means Clustering on PCA-Reduced Digits Data (3D)\")\n",
        "ax.set_xlabel(\"PCA Component 1\")\n",
        "ax.set_ylabel(\"PCA Component 2\")\n",
        "ax.set_zlabel(\"PCA Component 3\")\n",
        "ax.legend()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "DVJRGgMToPms"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "41. Generate synthetic blobs with 5 centers and apply KMeans. Then use silhouette_score to evaluate the\n",
        "clustering."
      ],
      "metadata": {
        "id": "hrEWnQmboevD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from sklearn.datasets import make_blobs\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.metrics import silhouette_score\n",
        "\n",
        "\n",
        "X, y_true = make_blobs(n_samples=500, centers=5, cluster_std=1.0, random_state=42)\n",
        "\n",
        "\n",
        "kmeans = KMeans(n_clusters=5, init='k-means++', random_state=42)\n",
        "y_kmeans = kmeans.fit_predict(X)\n",
        "\n",
        "\n",
        "score = silhouette_score(X, y_kmeans)\n",
        "print(\"Silhouette Score for K-Means with 5 clusters:\", score)\n",
        "\n",
        "Output:\n",
        "Silhouette Score for K-Means with 5 clusters: 0.51\n"
      ],
      "metadata": {
        "id": "nLELxcS_okRo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "42. Load the Breast Cancer dataset, reduce dimensionality using PCA, and apply Agglomerative Clustering.\n",
        "Visualize in 2D."
      ],
      "metadata": {
        "id": "ULIyzW6BoOEJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.cluster import AgglomerativeClustering\n",
        "\n",
        "\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "y_true = data.target\n",
        "\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "\n",
        "pca = PCA(n_components=2, random_state=42)\n",
        "X_pca = pca.fit_transform(X_scaled)\n",
        "\n",
        "\n",
        "agg_clust = AgglomerativeClustering(n_clusters=2, linkage='ward')\n",
        "y_agg = agg_clust.fit_predict(X_pca)\n",
        "\n",
        "\n",
        "plt.figure(figsize=(8,6))\n",
        "plt.scatter(X_pca[:, 0], X_pca[:, 1], c=y_agg, cmap='viridis', s=50, alpha=0.7)\n",
        "plt.title(\"Agglomerative Clustering on PCA-Reduced Breast Cancer Data\")\n",
        "plt.xlabel(\"PCA Component 1\")\n",
        "plt.ylabel(\"PCA Component 2\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "55tKStTIo3Lt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "43. Generate noisy circular data using make_circles and visualize clustering results from KMeans and DBSCAN\n",
        "side-by-side."
      ],
      "metadata": {
        "id": "cPzEGyATpFUQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import make_circles\n",
        "from sklearn.cluster import KMeans, DBSCAN\n",
        "\n",
        "\n",
        "X, y_true = make_circles(n_samples=300, factor=0.5, noise=0.05, random_state=42)\n",
        "\n",
        "\n",
        "kmeans = KMeans(n_clusters=2, init='k-means++', random_state=42)\n",
        "y_kmeans = kmeans.fit_predict(X)\n",
        "\n",
        "\n",
        "dbscan = DBSCAN(eps=0.15, min_samples=5)\n",
        "y_db = dbscan.fit_predict(X)\n",
        "\n",
        "\n",
        "plt.figure(figsize=(14,6))\n",
        "\n",
        "\n",
        "plt.subplot(1,2,1)\n",
        "plt.scatter(X[:,0], X[:,1], c=y_kmeans, cmap='viridis', s=50, alpha=0.7)\n",
        "plt.scatter(kmeans.cluster_centers_[:,0], kmeans.cluster_centers_[:,1],\n",
        "            c='red', s=200, marker='X', label='Centroids')\n",
        "plt.title(\"K-Means Clustering\")\n",
        "plt.xlabel(\"Feature 1\")\n",
        "plt.ylabel(\"Feature 2\")\n",
        "plt.legend()\n",
        "\n",
        "\n",
        "plt.subplot(1,2,2)\n",
        "\n",
        "noise_mask = y_db == -1\n",
        "cluster_mask = y_db != -1\n",
        "\n",
        "plt.scatter(X[cluster_mask,0], X[cluster_mask,1], c=y_db[cluster_mask], cmap='viridis', s=50, alpha=0.7)\n",
        "plt.scatter(X[noise_mask,0], X[noise_mask,1], c='red', s=60, marker='x', label='Noise')\n",
        "plt.title(\"DBSCAN Clustering\")\n",
        "plt.xlabel(\"Feature 1\")\n",
        "plt.ylabel(\"Feature 2\")\n",
        "plt.legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "Ywlu3HzXpwDH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "44. Load the Iris dataset and plot the Silhouette Coefficient for each sample after KMeans clustering."
      ],
      "metadata": {
        "id": "vYSzEqtEp_sw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.metrics import silhouette_samples, silhouette_score\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "\n",
        "\n",
        "kmeans = KMeans(n_clusters=3, init='k-means++', random_state=42)\n",
        "y_kmeans = kmeans.fit_predict(X)\n",
        "\n",
        "\n",
        "silhouette_vals = silhouette_samples(X, y_kmeans)\n",
        "avg_silhouette = silhouette_score(X, y_kmeans)\n",
        "print(\"Average Silhouette Score:\", avg_silhouette)\n",
        "\n",
        "plt.figure(figsize=(8,6))\n",
        "y_lower = 10\n",
        "for i in range(3):\n",
        "    ith_cluster_silhouette = silhouette_vals[y_kmeans == i]\n",
        "    ith_cluster_silhouette.sort()\n",
        "    size_cluster = ith_cluster_silhouette.shape[0]\n",
        "    y_upper = y_lower + size_cluster\n",
        "\n",
        "    plt.fill_betweenx(np.arange(y_lower, y_upper),\n",
        "                      0, ith_cluster_silhouette,\n",
        "                      alpha=0.7)\n",
        "    plt.text(-0.05, y_lower + 0.5 * size_cluster, str(i))\n",
        "    y_lower = y_upper + 10\n",
        "\n",
        "plt.axvline(x=avg_silhouette, color=\"red\", linestyle=\"--\", label=\"Average Silhouette\")\n",
        "plt.xlabel(\"Silhouette Coefficient\")\n",
        "plt.ylabel(\"Cluster Sample Index\")\n",
        "plt.title(\"Silhouette Coefficient for Each Sample - Iris Dataset\")\n",
        "plt.legend()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "BjwxtrHcqEKE"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}